{
 "cells": [
  {
   "cell_type": "code",
   "id": "4fbcbe57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:49:32.387467Z",
     "start_time": "2025-10-05T04:49:32.251016Z"
    }
   },
   "source": "%pip -q install pandas numpy scikit-learn matplotlib python-dateutil xgboost lightgbm tensorflow==2.*",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: tensorflow==2.*\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "5d5e9f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:49:32.929493Z",
     "start_time": "2025-10-05T04:49:32.925094Z"
    }
   },
   "source": [
    "import os, sys, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_DIR = \"/Users/llouis/Documents/model_test\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "sys.path.insert(0, BASE_DIR)\n",
    "from pipeline import run_pipeline\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/llouis/Documents/model_test\n",
      "DATA_DIR: /Users/llouis/Documents/model_test/data\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "04b7b532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:49:33.760845Z",
     "start_time": "2025-10-05T04:49:33.570210Z"
    }
   },
   "source": [
    "def read_csv_smart(path):\n",
    "    for enc in [\"utf-8\", \"cp949\", \"euc-kr\", \"latin1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(f\"CSV 인코딩 해석 실패: {path}\")\n",
    "\n",
    "\n",
    "ds1 = read_csv_smart(os.path.join(DATA_DIR, \"big_data_set1_f.csv\"))\n",
    "ds2 = read_csv_smart(os.path.join(DATA_DIR, \"ds2_monthly_usage.csv\"))\n",
    "ds3 = read_csv_smart(os.path.join(DATA_DIR, \"ds3_monthly_customers.csv\"))\n",
    "print(ds1.shape, ds2.shape, ds3.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4185, 9) (86590, 15) (86590, 17)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "24f089cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:49:35.464072Z",
     "start_time": "2025-10-05T04:49:34.385243Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "KEY_MCT, KEY_YM = \"ENCODED_MCT\", \"TA_YM\"\n",
    "\n",
    "\n",
    "def to_month(s):\n",
    "    dt = pd.to_datetime(s.astype(str), errors=\"coerce\")\n",
    "    return pd.to_datetime(dt.dt.to_period(\"M\").astype(str))\n",
    "\n",
    "\n",
    "def build_labels_robust(ds1, ds2, ds3, k_months=3, topq=0.10):\n",
    "    df = ds2.merge(ds3, on=[KEY_MCT, KEY_YM], how=\"outer\")\n",
    "    df[KEY_YM] = to_month(df[KEY_YM])\n",
    "    df[KEY_MCT] = df[KEY_MCT].astype(str)\n",
    "    df = df.sort_values([KEY_MCT, KEY_YM]).reset_index(drop=True)\n",
    "    df[\"y\"] = 0\n",
    "\n",
    "    # 1) Real label (폐업일)\n",
    "    if \"MCT_ME_D\" in ds1.columns:\n",
    "        tmp = ds1[[KEY_MCT, \"MCT_ME_D\"]].copy()\n",
    "        tmp[KEY_MCT] = tmp[KEY_MCT].astype(str)\n",
    "        tmp[\"MCT_ME_D\"] = pd.to_datetime(tmp[\"MCT_ME_D\"], errors=\"coerce\")\n",
    "        df = df.merge(tmp, on=KEY_MCT, how=\"left\")\n",
    "        t0 = df[KEY_YM]\n",
    "        tK = t0 + pd.offsets.MonthEnd(0) + pd.DateOffset(months=k_months)\n",
    "        cond = (df[\"MCT_ME_D\"].notna()) & (df[\"MCT_ME_D\"] > t0) & (df[\"MCT_ME_D\"] <= tK)\n",
    "        df.loc[cond, \"y\"] = 1\n",
    "\n",
    "    # 2) Proxy label (급락/해지 과열)\n",
    "    if df[\"y\"].nunique() < 2:\n",
    "        def bin2num(s):\n",
    "            s = s.astype(str)\n",
    "            m = s.str.extract(r\"(\\d+)\", expand=False)\n",
    "            return pd.to_numeric(m, errors=\"coerce\")\n",
    "\n",
    "        df[\"RC_SAA_num\"] = bin2num(df.get(\"RC_M1_SAA\", \"\"))\n",
    "        df[\"RC_CUS_num\"] = bin2num(df.get(\"RC_M1_UE_CUS_CN\", \"\"))\n",
    "        df[\"dSAA\"] = df.groupby(KEY_MCT)[\"RC_SAA_num\"].diff()\n",
    "        df[\"dCUS\"] = df.groupby(KEY_MCT)[\"RC_CUS_num\"].diff()\n",
    "        cxl = pd.to_numeric(df.get(\"APV_CE_RAT\", 0), errors=\"coerce\")\n",
    "        indme = pd.to_numeric(df.get(\"M12_SME_RY_ME_MCT_RAT\", 0), errors=\"coerce\")\n",
    "        bznme = pd.to_numeric(df.get(\"M12_SME_BZN_ME_MCT_RAT\", 0), errors=\"coerce\")\n",
    "        sig = (df[\"dSAA\"] <= -10).astype(int) + (df[\"dCUS\"] <= -10).astype(int) + (cxl >= 90).astype(int) + (\n",
    "                indme >= 80).astype(int) + (bznme >= 80).astype(int)\n",
    "        df[\"y_proxy\"] = (sig >= 2).astype(int)\n",
    "        if df[\"y_proxy\"].nunique() >= 2 and df[\"y_proxy\"].sum() > 0:\n",
    "            df[\"y\"] = df[\"y_proxy\"]\n",
    "\n",
    "    # 3) 여전히 한 클래스면 risk 기반 topq\n",
    "    if df[\"y\"].nunique() < 2:\n",
    "        out = run_pipeline(ds1, ds2, ds3, preds=None)\n",
    "        outj = out.merge(df[[KEY_MCT, KEY_YM]], on=[KEY_MCT, KEY_YM], how=\"right\")\n",
    "        pf = pd.to_numeric(outj[\"p_final\"], errors=\"coerce\").fillna(0)\n",
    "        thr = pf.quantile(1 - topq)\n",
    "        df[\"y\"] = (pf >= thr).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "robust_df = build_labels_robust(ds1, ds2, ds3, k_months=3, topq=0.10)\n",
    "print(\"Label counts:\", robust_df[\"y\"].value_counts(dropna=False).to_dict())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: {1: 86590}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "926b2619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:49:36.247527Z",
     "start_time": "2025-10-05T04:49:36.239281Z"
    }
   },
   "source": [
    "num_cols = [\n",
    "    \"M1_SME_RY_SAA_RAT\", \"M1_SME_RY_CNT_RAT\",\n",
    "    \"M12_SME_RY_SAA_PCE_RT\", \"M12_SME_BZN_SAA_PCE_RT\",\n",
    "    \"M12_SME_RY_ME_MCT_RAT\", \"M12_SME_BZN_ME_MCT_RAT\",\n",
    "    \"DLV_SAA_RAT\", \"MCT_UE_CLN_REU_RAT\", \"MCT_UE_CLN_NEW_RAT\"\n",
    "]\n",
    "cat_cols = [c for c in [\"HPSN_MCT_ZCD_NM\", \"HPSN_MCT_BZN_CD_NM\"] if c in robust_df.columns]\n",
    "\n",
    "X = robust_df[num_cols + cat_cols].copy()\n",
    "y = robust_df[\"y\"].astype(int)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_transform = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))])\n",
    "ct = ColumnTransformer([\n",
    "    (\"num\", num_transform, num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "], remainder=\"drop\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:49:38.993877Z",
     "start_time": "2025-10-05T04:49:38.985744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import ClassifierMixin\n",
    "import copy\n",
    "\n",
    "\n",
    "def stratified_split_force(X, y, test_size=0.25, random_state=42):\n",
    "    y = y.reset_index(drop=True)\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "\n",
    "    if len(idx_pos) >= 2 and len(idx_neg) >= 2:\n",
    "        rs = np.random.RandomState(random_state)\n",
    "        t_pos = max(1, int(round(test_size * len(idx_pos))))\n",
    "        t_neg = max(1, int(round(test_size * len(idx_neg))))\n",
    "        test_idx = np.r_[rs.choice(idx_pos, t_pos, replace=False),\n",
    "        rs.choice(idx_neg, t_neg, replace=False)]\n",
    "        train_mask = np.ones(len(y), dtype=bool)\n",
    "        train_mask[test_idx] = False\n",
    "        return X.iloc[train_mask], X.iloc[test_idx], y.iloc[train_mask], y.iloc[test_idx]\n",
    "    else:\n",
    "        return X, X, y, y\n",
    "\n",
    "\n",
    "def safe_proba(pipe, X, pos_label=1):\n",
    "    clf = None\n",
    "    try:\n",
    "        clf = pipe.named_steps.get(\"clf\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(X)\n",
    "        classes_ = None\n",
    "        if clf is not None and hasattr(clf, \"classes_\"):\n",
    "            classes_ = clf.classes_\n",
    "        elif hasattr(pipe, \"classes_\"):\n",
    "            classes_ = pipe.classes_\n",
    "\n",
    "        if proba.shape[1] == 1:\n",
    "            cls = classes_[0] if classes_ is not None and len(classes_) == 1 else None\n",
    "            if cls == pos_label:\n",
    "                return np.ones(proba.shape[0])\n",
    "            else:\n",
    "                return np.zeros(proba.shape[0])\n",
    "        else:\n",
    "            if classes_ is not None:\n",
    "                idx = int(np.where(classes_ == pos_label)[0][0])\n",
    "            else:\n",
    "                idx = 1\n",
    "            return proba[:, idx]\n",
    "\n",
    "    if hasattr(pipe, \"decision_function\"):\n",
    "        s = pipe.decision_function(X)\n",
    "        return 1.0 / (1.0 + np.exp(-s))\n",
    "\n",
    "    pred = pipe.predict(X)\n",
    "    return pred.astype(float)"
   ],
   "id": "774ce351d3e2c32a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "ae38d110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:49:42.836222Z",
     "start_time": "2025-10-05T04:49:41.704954Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def metrics_safe(y_true, p):\n",
    "    if hasattr(y_true, \"nunique\"):\n",
    "        nuniq = y_true.nunique()\n",
    "    else:\n",
    "        nuniq = len(np.unique(y_true))\n",
    "    if nuniq < 2:\n",
    "        return {\"note\": \"single-class test; AUC/PR 미정\", \"roc_auc\": None, \"pr_auc\": None}\n",
    "    return {\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, p)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, p)),\n",
    "    }\n",
    "\n",
    "\n",
    "Xtr, Xte, ytr, yte = stratified_split_force(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# ====== A. 전체가 한 클래스인 경우 → 비지도(IF) 경로로 통째 처리 ======\n",
    "if y.nunique() < 2:\n",
    "    print(\"Global single-class detected → IsolationForest-only fallback.\")\n",
    "\n",
    "    ct_dl = copy.deepcopy(ct)\n",
    "    pipe_if = Pipeline([(\"prep\", ct_dl), (\"clf\", IsolationForest(\n",
    "        n_estimators=400, contamination=0.10, random_state=42, n_jobs=-1\n",
    "    ))])\n",
    "    pipe_if.fit(X)\n",
    "    s = pipe_if[\"clf\"].score_samples(pipe_if[\"prep\"].transform(X))\n",
    "    s = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "    pe = 1 - s\n",
    "\n",
    "    prf_f = pgb_f = pxgb_f = plgb_f = pdl_f = pe\n",
    "\n",
    "    preds_full = robust_df[[\"ENCODED_MCT\", \"TA_YM\"]].copy()\n",
    "    preds_full[\"ENCODED_MCT\"] = preds_full[\"ENCODED_MCT\"].astype(str)\n",
    "    preds_full[\"TA_YM\"] = pd.to_datetime(preds_full[\"TA_YM\"], errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "    preds_full[\"pred_xgb\"] = pxgb_f\n",
    "    preds_full[\"pred_lgbm\"] = plgb_f\n",
    "    preds_full[\"pred_rf\"] = prf_f\n",
    "    preds_full[\"pred_gb\"] = pgb_f\n",
    "    preds_full[\"pred_dl\"] = pdl_f\n",
    "    preds_full = preds_full.dropna(subset=[\"ENCODED_MCT\", \"TA_YM\"])\n",
    "    preds_path = os.path.join(DATA_DIR, \"preds.csv\")\n",
    "    preds_full.to_csv(preds_path, index=False, encoding=\"utf-8\")\n",
    "    print(\"Saved preds:\", preds_path)\n",
    "\n",
    "else:\n",
    "    # ====== B. 지도 경로(두 클래스 존재) — 개별 모델 안전 학습 ======\n",
    "    def fit_or_dummy(pipe_ctor, Xtr, ytr, Xte, label_name):\n",
    "        try:\n",
    "            if ytr.nunique() < 2:\n",
    "                raise ValueError(\"train has single class\")\n",
    "            pipe = pipe_ctor()\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            p = safe_proba(pipe, Xte)\n",
    "            return pipe, p, False\n",
    "        except Exception as e:\n",
    "            print(f\"[{label_name}] fallback to DummyClassifier due to: {e}\")\n",
    "            dummy = Pipeline([\n",
    "                (\"prep\", ct),\n",
    "                (\"clf\", DummyClassifier(strategy=\"prior\"))\n",
    "            ])\n",
    "            dummy.fit(Xtr, ytr)\n",
    "            p = safe_proba(dummy, Xte)\n",
    "            return dummy, p, True\n",
    "\n",
    "\n",
    "    # 1) RF\n",
    "    rf_ctor = lambda: Pipeline([(\"prep\", ct),\n",
    "                                (\"clf\", RandomForestClassifier(\n",
    "                                    n_estimators=400, random_state=42, n_jobs=-1, class_weight=\"balanced\"))])\n",
    "    rf, prf, rf_dummy = fit_or_dummy(rf_ctor, Xtr, ytr, Xte, \"RF\")\n",
    "    print(\"RF :\", metrics_safe(yte, prf))\n",
    "\n",
    "    # 2) GB\n",
    "    gb_ctor = lambda: Pipeline([(\"prep\", ct),\n",
    "                                (\"clf\", GradientBoostingClassifier(random_state=42))])\n",
    "    gb, pgb, gb_dummy = fit_or_dummy(gb_ctor, Xtr, ytr, Xte, \"GB\")\n",
    "    print(\"GB :\", metrics_safe(yte, pgb))\n",
    "\n",
    "    # 3) XGB\n",
    "    xgb_ctor = lambda: Pipeline([(\"prep\", ct),\n",
    "                                 (\"clf\", xgb.XGBClassifier(\n",
    "                                     n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "                                     subsample=0.8, colsample_bytree=0.8,\n",
    "                                     eval_metric=\"logloss\", random_state=42, tree_method=\"hist\"))])\n",
    "    xgb_clf, pxgb, xgb_dummy = fit_or_dummy(xgb_ctor, Xtr, ytr, Xte, \"XGB\")\n",
    "    print(\"XGB:\", metrics_safe(yte, pxgb))\n",
    "\n",
    "    # 4) LGB\n",
    "    lgb_ctor = lambda: Pipeline([(\"prep\", ct),\n",
    "                                 (\"clf\", lgb.LGBMClassifier(\n",
    "                                     n_estimators=500, max_depth=-1, num_leaves=31, learning_rate=0.05,\n",
    "                                     subsample=0.8, colsample_bytree=0.8,\n",
    "                                     objective=\"binary\", random_state=42))])\n",
    "    lgb_clf, plgb, lgb_dummy = fit_or_dummy(lgb_ctor, Xtr, ytr, Xte, \"LGB\")\n",
    "    print(\"LGB:\", metrics_safe(yte, plgb))\n",
    "\n",
    "    # 5) DL (별도 전처리 복사본 사용)\n",
    "    ct_dl = copy.deepcopy(ct)\n",
    "    Xd_tr = ct_dl.fit_transform(Xtr)\n",
    "    Xd_te = ct_dl.transform(Xte)\n",
    "\n",
    "    dl_model = None\n",
    "    try:\n",
    "        if ytr.nunique() < 2:\n",
    "            raise ValueError(\"train has single class\")\n",
    "        inp = keras.Input(shape=(Xd_tr.shape[1],))\n",
    "        h = layers.Dense(128, activation=\"relu\")(inp)\n",
    "        h = layers.Dropout(0.2)(h)\n",
    "        h = layers.Dense(64, activation=\"relu\")(h)\n",
    "        outp = layers.Dense(1, activation=\"sigmoid\")(h)\n",
    "        dl_model = keras.Model(inp, outp)\n",
    "        dl_model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"binary_crossentropy\")\n",
    "        dl_model.fit(Xd_tr, ytr, epochs=10, batch_size=256, verbose=0)\n",
    "        pdl = dl_model.predict(Xd_te, verbose=0).ravel()\n",
    "        print(\"DL :\", metrics_safe(yte, pdl))\n",
    "        dl_dummy = False\n",
    "    except Exception as e:\n",
    "        print(f\"[DL] fallback to Dummy due to: {e}\")\n",
    "        dummy_dl = Pipeline([(\"prep\", ct), (\"clf\", DummyClassifier(strategy=\"prior\"))])\n",
    "        dummy_dl.fit(Xtr, ytr)\n",
    "        pdl = safe_proba(dummy_dl, Xte)\n",
    "        dl_dummy = True\n",
    "\n",
    "    # ===== 앙상블 + 보정 (이용 가능한 모델만 사용해 가중 재정규화) =====\n",
    "    w = {\"xgb\": 0.25, \"lgb\": 0.25, \"rf\": 0.25, \"gb\": 0.15, \"dl\": 0.10}\n",
    "    parts, used = [], []\n",
    "    if pxgb is not None: parts.append((w[\"xgb\"], pxgb)); used.append(\"xgb\")\n",
    "    if plgb is not None: parts.append((w[\"lgb\"], plgb)); used.append(\"lgb\")\n",
    "    if prf is not None: parts.append((w[\"rf\"], prf));  used.append(\"rf\")\n",
    "    if pgb is not None: parts.append((w[\"gb\"], pgb));  used.append(\"gb\")\n",
    "    if pdl is not None: parts.append((w[\"dl\"], pdl));  used.append(\"dl\")\n",
    "    sw = sum(w for w, _ in parts)\n",
    "    stack = np.sum([(w_ / sw) * p for (w_, p) in parts], axis=0)\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    if yte.nunique() >= 2:\n",
    "        pl = LogisticRegression(max_iter=200)\n",
    "        pl.fit(stack.reshape(-1, 1), yte)\n",
    "        stack_cal = pl.predict_proba(stack.reshape(-1, 1))[:, 1]\n",
    "        print(\"Ensemble raw:\", metrics_safe(yte, stack))\n",
    "        print(\"Ensemble cal:\", metrics_safe(yte, stack_cal))\n",
    "    else:\n",
    "        stack_cal = stack\n",
    "\n",
    "\n",
    "    # ===== 전체 데이터 예측 → preds.csv =====\n",
    "    def predict_full_models():\n",
    "        prf_f = safe_proba(rf, X) if rf is not None else stack\n",
    "        pgb_f = safe_proba(gb, X) if gb is not None else stack\n",
    "        pxgb_f = safe_proba(xgb_clf, X) if xgb_clf is not None else stack\n",
    "        plgb_f = safe_proba(lgb_clf, X) if lgb_clf is not None else stack\n",
    "        Xd_full = ct_dl.transform(X)\n",
    "        if dl_model is not None and not dl_dummy:\n",
    "            pdl_f = dl_model.predict(Xd_full, verbose=0).ravel()\n",
    "        else:\n",
    "            pdl_f = stack\n",
    "        if yte.nunique() >= 2:\n",
    "            stack_f = (0.25 * pxgb_f + 0.25 * plgb_f + 0.25 * prf_f + 0.15 * pgb_f + 0.10 * pdl_f)\n",
    "            pcal_f = pl.predict_proba(stack_f.reshape(-1, 1))[:, 1]\n",
    "        else:\n",
    "            pcal_f = (0.25 * pxgb_f + 0.25 * plgb_f + 0.25 * prf_f + 0.15 * pgb_f + 0.10 * pdl_f)\n",
    "        return prf_f, pgb_f, pxgb_f, plgb_f, pdl_f, pcal_f\n",
    "\n",
    "\n",
    "    prf_f, pgb_f, pxgb_f, plgb_f, pdl_f, pcal_f = predict_full_models()\n",
    "\n",
    "    preds_full = robust_df[[\"ENCODED_MCT\", \"TA_YM\"]].copy()\n",
    "    preds_full[\"ENCODED_MCT\"] = preds_full[\"ENCODED_MCT\"].astype(str)\n",
    "    preds_full[\"TA_YM\"] = pd.to_datetime(preds_full[\"TA_YM\"], errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "    preds_full[\"pred_xgb\"] = pxgb_f\n",
    "    preds_full[\"pred_lgbm\"] = plgb_f\n",
    "    preds_full[\"pred_rf\"] = prf_f\n",
    "    preds_full[\"pred_gb\"] = pgb_f\n",
    "    preds_full[\"pred_dl\"] = pdl_f\n",
    "    preds_full = preds_full.dropna(subset=[\"ENCODED_MCT\", \"TA_YM\"])\n",
    "    preds_path = os.path.join(DATA_DIR, \"preds.csv\")\n",
    "    preds_full.to_csv(preds_path, index=False, encoding=\"utf-8\")\n",
    "    print(\"Saved preds:\", preds_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global single-class detected → IsolationForest-only fallback.\n",
      "Saved preds: /Users/llouis/Documents/model_test/data/preds.csv\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "ecf3a3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:49:43.821091Z",
     "start_time": "2025-10-05T04:49:43.732956Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "\n",
    "def _fallback_split(X, y, test_size=0.25, random_state=42):\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        return train_test_split(X, y, stratify=y if getattr(y, \"nunique\", lambda: 2)() > 1 else None,\n",
    "                                test_size=test_size, random_state=random_state)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"X/y로 테스트 분할을 만들 수 없습니다. 상위 셀에서 X, y를 먼저 정의하세요.\") from e\n",
    "\n",
    "\n",
    "def _is_fitted_estimator(est) -> bool:\n",
    "    try:\n",
    "        check_is_fitted(est)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _is_fitted_pipeline(pipe) -> bool:\n",
    "    if pipe is None:\n",
    "        return False\n",
    "    est = getattr(pipe, \"named_steps\", {}).get(\"clf\", pipe)\n",
    "    return _is_fitted_estimator(est)\n",
    "\n",
    "\n",
    "def _safe_proba(pipe, X, pos_label=1):\n",
    "    n = len(X)\n",
    "    if pipe is None or not _is_fitted_pipeline(pipe):\n",
    "        return np.zeros(n)\n",
    "\n",
    "    clf = getattr(pipe, \"named_steps\", {}).get(\"clf\", None)\n",
    "\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        try:\n",
    "            proba = pipe.predict_proba(X)\n",
    "            if proba.ndim == 1:\n",
    "                return proba.astype(float)\n",
    "            if proba.shape[1] > 1:\n",
    "                classes_ = getattr(clf, \"classes_\", getattr(pipe, \"classes_\", None))\n",
    "                if classes_ is not None and pos_label in list(classes_):\n",
    "                    idx = int(np.where(classes_ == pos_label)[0][0])\n",
    "                else:\n",
    "                    idx = 1\n",
    "                return proba[:, idx]\n",
    "            else:\n",
    "                classes_ = getattr(clf, \"classes_\", getattr(pipe, \"classes_\", [0]))\n",
    "                return np.ones(n) if (len(classes_) == 1 and classes_[0] == pos_label) else np.zeros(n)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if hasattr(pipe, \"decision_function\"):\n",
    "        try:\n",
    "            s = pipe.decision_function(X)\n",
    "            return 1.0 / (1.0 + np.exp(-s))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        pred = pipe.predict(X)\n",
    "        return pred.astype(float)\n",
    "    except Exception:\n",
    "        return np.zeros(n)\n",
    "\n",
    "\n",
    "if \"Xte\" not in globals() or \"yte\" not in globals():\n",
    "    if \"X\" not in globals() or \"y\" not in globals():\n",
    "        raise RuntimeError(\"앙상블/보정을 위해서는 상위 셀에서 X, y가 먼저 정의되어 있어야 합니다.\")\n",
    "    if \"stratified_split_force\" in globals():\n",
    "        Xtr, Xte, ytr, yte = stratified_split_force(X, y, test_size=0.25, random_state=42)\n",
    "    else:\n",
    "        Xtr, Xte, ytr, yte = _fallback_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "prf = _safe_proba(globals().get(\"rf\", None), Xte)\n",
    "pgb = _safe_proba(globals().get(\"gb\", None), Xte)\n",
    "pxgb = _safe_proba(globals().get(\"xgb_clf\", None), Xte)\n",
    "plgb = _safe_proba(globals().get(\"lgb_clf\", None), Xte)\n",
    "\n",
    "if \"dl_model\" in globals() and \"ct_dl\" in globals() and dl_model is not None:\n",
    "    try:\n",
    "        Xd_te = ct_dl.transform(Xte)\n",
    "        pdl = dl_model.predict(Xd_te, verbose=0).ravel()\n",
    "    except Exception:\n",
    "        pdl = np.zeros(len(Xte))\n",
    "else:\n",
    "    pdl = np.zeros(len(Xte))\n",
    "\n",
    "\n",
    "def metrics_safe(y_true, p):\n",
    "    y_true = pd.Series(y_true)\n",
    "    if y_true.nunique() < 2:\n",
    "        return {\"note\": \"single-class test; AUC/PR 미정\", \"roc_auc\": None, \"pr_auc\": None}\n",
    "    return {\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, p)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, p)),\n",
    "    }\n",
    "\n",
    "\n",
    "def platt_calibrate(scores, y, max_iter=200):\n",
    "    scores = np.asarray(scores, dtype=\"float64\")\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    mask = np.isfinite(scores)\n",
    "    if not mask.all():\n",
    "        med = np.nanmedian(scores[mask]) if mask.any() else 0.5\n",
    "        scores[~mask] = med\n",
    "\n",
    "    if np.allclose(scores, scores[0]):\n",
    "        return scores, {\"note\": \"constant scores; skip calibration\"}\n",
    "    if pd.Series(y).nunique() < 2:\n",
    "        return scores, {\"note\": \"single-class; skip calibration\"}\n",
    "\n",
    "    lr = LogisticRegression(max_iter=max_iter)\n",
    "    lr.fit(scores.reshape(-1, 1), y)\n",
    "    calibrated = lr.predict_proba(scores.reshape(-1, 1))[:, 1]\n",
    "    info = {\"coef\": float(lr.coef_[0, 0]), \"intercept\": float(lr.intercept_[0])}\n",
    "    return calibrated, info\n",
    "\n",
    "\n",
    "w = {\"xgb\": 0.25, \"lgb\": 0.25, \"rf\": 0.25, \"gb\": 0.15, \"dl\": 0.10}\n",
    "stack = w[\"xgb\"] * pxgb + w[\"lgb\"] * plgb + w[\"rf\"] * prf + w[\"gb\"] * pgb + w[\"dl\"] * pdl\n",
    "\n",
    "stack_cal, pl_info = platt_calibrate(stack, yte, max_iter=200)\n",
    "print(\"Ensemble raw:\", metrics_safe(yte, stack))\n",
    "print(\"Ensemble cal:\", metrics_safe(yte, stack_cal))\n",
    "print(\"Platt info:\", pl_info)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble raw: {'note': 'single-class test; AUC/PR 미정', 'roc_auc': None, 'pr_auc': None}\n",
      "Ensemble cal: {'note': 'single-class test; AUC/PR 미정', 'roc_auc': None, 'pr_auc': None}\n",
      "Platt info: {'note': 'constant scores; skip calibration'}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "d3944339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:52:37.797205Z",
     "start_time": "2025-10-05T04:52:37.794349Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_full_models_safe(Xf):\n",
    "    prf_f = _safe_proba(globals().get(\"rf\", None), Xf)\n",
    "    pgb_f = _safe_proba(globals().get(\"gb\", None), Xf)\n",
    "    pxgb_f = _safe_proba(globals().get(\"xgb_clf\", None), Xf)\n",
    "    plgb_f = _safe_proba(globals().get(\"lgb_clf\", None), Xf)\n",
    "\n",
    "    # DL\n",
    "    if \"dl_model\" in globals() and \"ct_dl\" in globals() and dl_model is not None:\n",
    "        try:\n",
    "            Xd_full = ct_dl.transform(Xf)\n",
    "            pdl_f = dl_model.predict(Xd_full, verbose=0).ravel()\n",
    "        except Exception:\n",
    "            pdl_f = np.zeros(len(Xf))\n",
    "    else:\n",
    "        pdl_f = np.zeros(len(Xf))\n",
    "\n",
    "    w = {\"xgb\": 0.25, \"lgb\": 0.25, \"rf\": 0.25, \"gb\": 0.15, \"dl\": 0.10}\n",
    "    stack_f = (w[\"xgb\"] * pxgb_f + w[\"lgb\"] * plgb_f + w[\"rf\"] * prf_f + w[\"gb\"] * pgb_f + w[\"dl\"] * pdl_f)\n",
    "\n",
    "    use_platt = (\"pl\" in globals()) and (callable(getattr(pl, \"predict_proba\", None))) \\\n",
    "                and (\"yte\" in globals()) and (pd.Series(yte).nunique() >= 2)\n",
    "    if use_platt:\n",
    "        try:\n",
    "            pcal_f = pl.predict_proba(stack_f.reshape(-1, 1))[:, 1]\n",
    "        except Exception:\n",
    "            pcal_f = stack_f\n",
    "    else:\n",
    "        pcal_f = stack_f\n",
    "\n",
    "    return prf_f, pgb_f, pxgb_f, plgb_f, pdl_f, pcal_f"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "50ad8ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:52:41.319562Z",
     "start_time": "2025-10-05T04:52:40.016094Z"
    }
   },
   "source": [
    "p = pd.read_csv(preds_path)\n",
    "p[\"ENCODED_MCT\"] = p[\"ENCODED_MCT\"].astype(str)\n",
    "p[\"TA_YM\"] = pd.to_datetime(p[\"TA_YM\"], errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "p = p.dropna(subset=[\"ENCODED_MCT\", \"TA_YM\"])\n",
    "\n",
    "out = run_pipeline(ds1, ds2, ds3, preds=p)\n",
    "out_path = os.path.join(BASE_DIR, \"risk_output_trained.csv\")\n",
    "out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved:\", out_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/llouis/Documents/model_test/risk_output_trained.csv\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "30e34619af56e4f0"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
