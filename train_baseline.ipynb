{
 "cells": [
  {
   "cell_type": "code",
   "id": "c3cf4bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:01:24.026275Z",
     "start_time": "2025-10-05T04:01:23.636389Z"
    }
   },
   "source": [
    "%pip install -q pandas numpy scikit-learn matplotlib python-dateutil\n",
    "import os, sys, warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_DIR = \"/Users/llouis/Documents/model_test\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "from pipeline import run_pipeline\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "BASE_DIR: /Users/llouis/Documents/model_test\n",
      "DATA_DIR: /Users/llouis/Documents/model_test/data\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "eb39add1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:01:25.612376Z",
     "start_time": "2025-10-05T04:01:25.420308Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_csv_smart(path):\n",
    "    for enc in [\"utf-8\", \"cp949\", \"euc-kr\", \"latin1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(f\"CSV 인코딩 해석 실패: {path}\")\n",
    "\n",
    "\n",
    "ds1 = read_csv_smart(os.path.join(DATA_DIR, \"big_data_set1_f.csv\"))\n",
    "ds2 = read_csv_smart(os.path.join(DATA_DIR, \"ds2_monthly_usage.csv\"))\n",
    "ds3 = read_csv_smart(os.path.join(DATA_DIR, \"ds3_monthly_customers.csv\"))\n",
    "\n",
    "print(ds1.shape, ds2.shape, ds3.shape)\n",
    "print(ds2.head(2))\n",
    "print(ds3.head(2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4185, 9) (86590, 15) (86590, 17)\n",
      "  ENCODED_MCT   TA_YM MCT_OPE_MS_CN           RC_M1_SAA      RC_M1_TO_UE_CT  \\\n",
      "0  000F03E44A  202404      4_50-75%            5_75-90%            5_75-90%   \n",
      "1  000F03E44A  202312      4_50-75%  6_90%초과(하위 10% 이하)  6_90%초과(하위 10% 이하)   \n",
      "\n",
      "      RC_M1_UE_CUS_CN      RC_M1_AV_NP_AT APV_CE_RAT  DLV_SAA_RAT  \\\n",
      "0            5_75-90%            5_75-90%    1_상위1구간    -999999.9   \n",
      "1  6_90%초과(하위 10% 이하)  6_90%초과(하위 10% 이하)        NaN    -999999.9   \n",
      "\n",
      "   M1_SME_RY_SAA_RAT  M1_SME_RY_CNT_RAT  M12_SME_RY_SAA_PCE_RT  \\\n",
      "0                2.6               10.6                   93.8   \n",
      "1                0.0                0.0                   94.8   \n",
      "\n",
      "   M12_SME_BZN_SAA_PCE_RT  M12_SME_RY_ME_MCT_RAT  M12_SME_BZN_ME_MCT_RAT  \n",
      "0                    71.5                   16.7                     7.8  \n",
      "1                    73.4                   16.6                     7.2  \n",
      "  ENCODED_MCT   TA_YM  M12_MAL_1020_RAT  M12_MAL_30_RAT  M12_MAL_40_RAT  \\\n",
      "0  0305234DDB  202311               0.0             0.0           100.0   \n",
      "1  0495B069FF  202403               0.0             0.0           100.0   \n",
      "\n",
      "   M12_MAL_50_RAT  M12_MAL_60_RAT  M12_FME_1020_RAT  M12_FME_30_RAT  \\\n",
      "0             0.0             0.0               0.0             0.0   \n",
      "1             0.0             0.0               0.0             0.0   \n",
      "\n",
      "   M12_FME_40_RAT  M12_FME_50_RAT  M12_FME_60_RAT  MCT_UE_CLN_REU_RAT  \\\n",
      "0             0.0             0.0             0.0               100.0   \n",
      "1             0.0             0.0             0.0                25.0   \n",
      "\n",
      "   MCT_UE_CLN_NEW_RAT  RC_M1_SHC_RSD_UE_CLN_RAT  RC_M1_SHC_WP_UE_CLN_RAT  \\\n",
      "0                 0.0                 -999999.9                -999999.9   \n",
      "1                25.0                       0.0                      0.0   \n",
      "\n",
      "   RC_M1_SHC_FLP_UE_CLN_RAT  \n",
      "0                 -999999.9  \n",
      "1                     100.0  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "a7bd2b69",
   "metadata": {},
   "source": [
    "## 라벨 생성 전략\n",
    "1) **실라벨(권장)**: `ds1`에 `MCT_ME_D`(폐업일)가 있으면, 기준월 `t`에서 이후 `K`개월 내 폐업 발생 여부를 라벨로 사용합니다.\n",
    "2) **프록시 라벨**: `ds2/3`만 있는 경우, 급락 시나리오(예: 매출 구간/고객수 구간 MoM 큰 하락) 조합으로 이벤트를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "94dcaaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:01:28.184604Z",
     "start_time": "2025-10-05T04:01:28.020524Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "KEY_MCT = \"ENCODED_MCT\"\n",
    "KEY_YM = \"TA_YM\"\n",
    "K = 3\n",
    "\n",
    "\n",
    "def to_month(dt_series):\n",
    "    dt = pd.to_datetime(dt_series.astype(str), errors=\"coerce\")\n",
    "    return pd.to_datetime(dt.dt.to_period(\"M\").astype(str))\n",
    "\n",
    "\n",
    "df = ds2.merge(ds3, on=[KEY_MCT, KEY_YM], how=\"outer\")\n",
    "df[KEY_YM] = to_month(df[KEY_YM])\n",
    "df = df.sort_values([KEY_MCT, KEY_YM])\n",
    "\n",
    "use_real_label = False\n",
    "if \"MCT_ME_D\" in ds1.columns:\n",
    "    tmp = ds1[[KEY_MCT, \"MCT_ME_D\"]].copy()\n",
    "    tmp[\"MCT_ME_D\"] = pd.to_datetime(tmp[\"MCT_ME_D\"], errors=\"coerce\")\n",
    "    df = df.merge(tmp, on=KEY_MCT, how=\"left\")\n",
    "    df[\"y\"] = 0\n",
    "    me = df[\"MCT_ME_D\"]\n",
    "    t0 = df[KEY_YM]\n",
    "    tK = t0 + pd.offsets.MonthEnd(0) + pd.DateOffset(months=K)\n",
    "    cond = (me.notna()) & (me > t0) & (me <= tK)\n",
    "    df.loc[cond, \"y\"] = 1\n",
    "    use_real_label = df[\"y\"].sum() > 0\n",
    "\n",
    "if not use_real_label:\n",
    "    def bin2num(s):\n",
    "        s = s.astype(str)\n",
    "        m = s.str.extract(r\"(\\d+)\", expand=False)\n",
    "        return pd.to_numeric(m, errors=\"coerce\")\n",
    "\n",
    "\n",
    "    df[\"RC_SAA_num\"] = bin2num(df.get(\"RC_M1_SAA\", \"\"))\n",
    "    df[\"RC_CUS_num\"] = bin2num(df.get(\"RC_M1_UE_CUS_CN\", \"\"))\n",
    "\n",
    "    df = df.sort_values([KEY_MCT, KEY_YM])\n",
    "    df[\"dSAA\"] = df.groupby(KEY_MCT)[\"RC_SAA_num\"].diff()\n",
    "    df[\"dCUS\"] = df.groupby(KEY_MCT)[\"RC_CUS_num\"].diff()\n",
    "\n",
    "    cxl = pd.to_numeric(df.get(\"APV_CE_RAT\", 0), errors=\"coerce\")\n",
    "    ind_me = pd.to_numeric(df.get(\"M12_SME_RY_ME_MCT_RAT\", 0), errors=\"coerce\")\n",
    "    bzn_me = pd.to_numeric(df.get(\"M12_SME_BZN_ME_MCT_RAT\", 0), errors=\"coerce\")\n",
    "\n",
    "    bad = (\n",
    "            (df[\"dSAA\"] <= -10) | (df[\"dCUS\"] <= -10) |\n",
    "            (cxl >= 90) | (ind_me >= 90) | (bzn_me >= 90)\n",
    "    )\n",
    "    df[\"y\"] = bad.astype(int)\n",
    "\n",
    "print(\"Positive ratio:\", df[\"y\"].mean())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive ratio: 0.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:01:31.140273Z",
     "start_time": "2025-10-05T04:01:30.163620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def build_labels_robust(ds1, ds2, ds3, base_dir, k_months=3, topq=0.10):\n",
    "    KEY_MCT, KEY_YM = \"ENCODED_MCT\", \"TA_YM\"\n",
    "\n",
    "    def to_month(dt_series):\n",
    "        dt = pd.to_datetime(dt_series.astype(str), errors=\"coerce\")\n",
    "        return pd.to_datetime(dt.dt.to_period(\"M\").astype(str))\n",
    "\n",
    "    df = ds2.merge(ds3, on=[KEY_MCT, KEY_YM], how=\"outer\")\n",
    "    df[KEY_YM] = to_month(df[KEY_YM])\n",
    "    df = df.sort_values([KEY_MCT, KEY_YM]).reset_index(drop=True)\n",
    "    df[\"y\"] = 0\n",
    "\n",
    "    used_real = False\n",
    "    if \"MCT_ME_D\" in ds1.columns:\n",
    "        tmp = ds1[[KEY_MCT, \"MCT_ME_D\"]].copy()\n",
    "        tmp[\"MCT_ME_D\"] = pd.to_datetime(tmp[\"MCT_ME_D\"], errors=\"coerce\")\n",
    "        df = df.merge(tmp, on=KEY_MCT, how=\"left\")\n",
    "        t0 = df[KEY_YM]\n",
    "        tK = t0 + pd.offsets.MonthEnd(0) + pd.DateOffset(months=k_months)\n",
    "        cond = (df[\"MCT_ME_D\"].notna()) & (df[\"MCT_ME_D\"] > t0) & (df[\"MCT_ME_D\"] <= tK)\n",
    "        df.loc[cond, \"y\"] = 1\n",
    "        used_real = df[\"y\"].sum() > 0\n",
    "\n",
    "    if df[\"y\"].nunique() < 2:\n",
    "        def bin2num(s):\n",
    "            s = s.astype(str)\n",
    "            m = s.str.extract(r\"(\\d+)\", expand=False)\n",
    "            return pd.to_numeric(m, errors=\"coerce\")\n",
    "\n",
    "        df[\"RC_SAA_num\"] = bin2num(df.get(\"RC_M1_SAA\", \"\"))\n",
    "        df[\"RC_CUS_num\"] = bin2num(df.get(\"RC_M1_UE_CUS_CN\", \"\"))\n",
    "\n",
    "        df[\"dSAA\"] = df.groupby(KEY_MCT)[\"RC_SAA_num\"].diff()\n",
    "        df[\"dCUS\"] = df.groupby(KEY_MCT)[\"RC_CUS_num\"].diff()\n",
    "\n",
    "        cxl = pd.to_numeric(df.get(\"APV_CE_RAT\", 0), errors=\"coerce\")\n",
    "        indme = pd.to_numeric(df.get(\"M12_SME_RY_ME_MCT_RAT\", 0), errors=\"coerce\")\n",
    "        bznme = pd.to_numeric(df.get(\"M12_SME_BZN_ME_MCT_RAT\", 0), errors=\"coerce\")\n",
    "\n",
    "        sig = 0\n",
    "        sig += (df[\"dSAA\"] <= -10).astype(int)\n",
    "        sig += (df[\"dCUS\"] <= -10).astype(int)\n",
    "        sig += (cxl >= 90).astype(int)\n",
    "        sig += (indme >= 80).astype(int)\n",
    "        sig += (bznme >= 80).astype(int)\n",
    "        df[\"y_proxy\"] = (sig >= 2).astype(int)\n",
    "\n",
    "        if df[\"y_proxy\"].nunique() >= 2 and df[\"y_proxy\"].sum() > 0:\n",
    "            df[\"y\"] = df[\"y_proxy\"]\n",
    "\n",
    "    if df[\"y\"].nunique() < 2:\n",
    "        sys.path.insert(0, base_dir)\n",
    "        from pipeline import run_pipeline\n",
    "\n",
    "        def read_csv_smart(path):\n",
    "            for enc in [\"utf-8\", \"cp949\", \"euc-kr\", \"latin1\"]:\n",
    "                try:\n",
    "                    return pd.read_csv(path, encoding=enc)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            raise RuntimeError(f\"CSV 인코딩 해석 실패: {path}\")\n",
    "\n",
    "        ds1_ = deepcopy(ds1)\n",
    "        ds2_ = deepcopy(ds2)\n",
    "        ds3_ = deepcopy(ds3)\n",
    "\n",
    "        out = run_pipeline(ds1_, ds2_, ds3_, preds=None)\n",
    "        outj = out.merge(df[[KEY_MCT, KEY_YM]], on=[KEY_MCT, KEY_YM], how=\"right\")\n",
    "        pf = pd.to_numeric(outj[\"p_final\"], errors=\"coerce\").fillna(0)\n",
    "        thr = pf.quantile(1 - topq)\n",
    "        df[\"y_q\"] = (pf >= thr).astype(int)\n",
    "\n",
    "        if df[\"y_q\"].nunique() >= 2 and df[\"y_q\"].sum() > 0:\n",
    "            df[\"y\"] = df[\"y_q\"]\n",
    "\n",
    "    if df[\"y\"].nunique() < 2:\n",
    "        if \"p_final\" not in df.columns:\n",
    "            df[\"p_final\"] = 0.0\n",
    "        idx = df[\"p_final\"].nlargest(min(50, len(df))).index\n",
    "        df.loc[idx, \"y\"] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "robust_df = build_labels_robust(ds1, ds2, ds3, BASE_DIR, k_months=3, topq=0.10)\n",
    "print(\"Label distribution:\", robust_df[\"y\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "KEY_MCT, KEY_YM = \"ENCODED_MCT\", \"TA_YM\"\n",
    "num_cols = [\n",
    "    \"M1_SME_RY_SAA_RAT\", \"M1_SME_RY_CNT_RAT\",\n",
    "    \"M12_SME_RY_SAA_PCE_RT\", \"M12_SME_BZN_SAA_PCE_RT\",\n",
    "    \"M12_SME_RY_ME_MCT_RAT\", \"M12_SME_BZN_ME_MCT_RAT\",\n",
    "    \"DLV_SAA_RAT\", \"MCT_UE_CLN_REU_RAT\", \"MCT_UE_CLN_NEW_RAT\"\n",
    "]\n",
    "cat_cols = [c for c in [\"HPSN_MCT_ZCD_NM\", \"HPSN_MCT_BZN_CD_NM\"] if c in robust_df.columns]\n",
    "\n",
    "X = robust_df[num_cols + cat_cols].copy()\n",
    "y = robust_df[\"y\"].astype(int)"
   ],
   "id": "e7bc6ab9307a851e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: {0: 86540, 1: 50}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "52d65174",
   "metadata": {},
   "source": [
    "## 특징 생성 & 학습/평가"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c3c792d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T04:02:24.970798Z",
     "start_time": "2025-10-05T04:02:16.140051Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "num_transform = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))])\n",
    "ct = ColumnTransformer([\n",
    "    (\"num\", num_transform, num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "\n",
    "def fit_supervised(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y if y.nunique() > 1 else None,\n",
    "        test_size=0.25, random_state=42\n",
    "    )\n",
    "    models = {\n",
    "        \"logit\": LogisticRegression(max_iter=200, class_weight=\"balanced\"),\n",
    "        \"rf\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1, class_weight=\"balanced\"),\n",
    "        \"gb\": GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    res, pipes, probs = {}, {}, {}\n",
    "    for name, clf in models.items():\n",
    "        pipe = Pipeline([(\"prep\", ct), (\"clf\", clf)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if hasattr(pipe, \"predict_proba\"):\n",
    "            p = pipe.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            s = pipe.decision_function(X_test)\n",
    "            p = 1 / (1 + np.exp(-s))\n",
    "        auc = roc_auc_score(y_test, p)\n",
    "        ap = average_precision_score(y_test, p)\n",
    "        res[name] = {\"roc_auc\": float(auc), \"pr_auc\": float(ap)}\n",
    "        pipes[name] = pipe\n",
    "        probs[name] = (X_test.index, p, y_test)\n",
    "    return res, pipes, probs\n",
    "\n",
    "\n",
    "def fit_oneclass(X):\n",
    "    pipe = Pipeline([(\"prep\", ct), (\"clf\", IsolationForest(\n",
    "        n_estimators=400, contamination=0.10, random_state=42, n_jobs=-1\n",
    "    ))])\n",
    "    pipe.fit(X)\n",
    "    s = pipe[\"clf\"].score_samples(pipe[\"prep\"].transform(X))\n",
    "    s = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "    p = 1 - s\n",
    "    return {\"oneclass\": {\"note\": \"IsolationForest\"}}, {\"oneclass\": pipe}, {\"oneclass\": (X.index, p, None)}\n",
    "\n",
    "\n",
    "if y.nunique() >= 2 and y.sum() > 0:\n",
    "    res, pipes, probs = fit_supervised(X, y)\n",
    "    print(\"Supervised metrics:\", res)\n",
    "else:\n",
    "    print(\"Only one class detected → using IsolationForest fallback.\")\n",
    "    res, pipes, probs = fit_oneclass(X)\n",
    "\n",
    "keys = list(pipes.keys())\n",
    "\n",
    "\n",
    "def predict_full(pipe_dict, Xf):\n",
    "    P = []\n",
    "    for k in pipe_dict:\n",
    "        pipe = pipe_dict[k]\n",
    "        if \"IsolationForest\" in str(pipe):\n",
    "            s = pipe[\"clf\"].score_samples(pipe[\"prep\"].transform(Xf))\n",
    "            s = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "            p = 1 - s\n",
    "        else:\n",
    "            if hasattr(pipe, \"predict_proba\"):\n",
    "                p = pipe.predict_proba(Xf)[:, 1]\n",
    "            else:\n",
    "                sc = pipe.decision_function(Xf)\n",
    "                p = 1 / (1 + np.exp(-sc))\n",
    "        P.append(p)\n",
    "    P = np.vstack(P).T\n",
    "    return P.mean(axis=1)\n",
    "\n",
    "\n",
    "X_full = robust_df[num_cols + cat_cols].copy()\n",
    "pe = predict_full(pipes, X_full)\n",
    "\n",
    "preds_full[\"ENCODED_MCT\"] = preds_full[\"ENCODED_MCT\"].astype(str)\n",
    "preds_full[\"TA_YM\"] = pd.to_datetime(preds_full[\"TA_YM\"], errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "preds_full[\"pred_xgb\"] = np.nan\n",
    "preds_full[\"pred_lgbm\"] = np.nan\n",
    "preds_full[\"pred_rf\"] = pe\n",
    "preds_full[\"pred_gb\"] = np.nan\n",
    "preds_full[\"pred_dl\"] = np.nan\n",
    "\n",
    "preds_path = os.path.join(DATA_DIR, \"preds.csv\")\n",
    "preds_full.to_csv(preds_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved preds:\", preds_path)\n",
    "\n",
    "out = run_pipeline(ds1, ds2, ds3, preds=pd.read_csv(preds_path))\n",
    "out_path = os.path.join(BASE_DIR, \"risk_output_trained.csv\")\n",
    "out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved:\", out_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised metrics: {'logit': {'roc_auc': 0.8744768981884766, 'pr_auc': 0.0028350413288156994}, 'rf': {'roc_auc': 0.9997155606122559, 'pr_auc': 0.8314446237523161}, 'gb': {'roc_auc': 0.6852002631064336, 'pr_auc': 0.05085954503504265}}\n",
      "Saved preds: /Users/llouis/Documents/model_test/data/preds.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on datetime64[ns] and float64 columns for key 'TA_YM'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 100\u001B[39m\n\u001B[32m     97\u001B[39m preds_full.to_csv(preds_path, index=\u001B[38;5;28;01mFalse\u001B[39;00m, encoding=\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     98\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mSaved preds:\u001B[39m\u001B[33m\"\u001B[39m, preds_path)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m out = \u001B[43mrun_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mds2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mds3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m out_path = os.path.join(BASE_DIR, \u001B[33m\"\u001B[39m\u001B[33mrisk_output_trained.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    102\u001B[39m out.to_csv(out_path, index=\u001B[38;5;28;01mFalse\u001B[39;00m, encoding=\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/model_test/pipeline.py:23\u001B[39m, in \u001B[36mrun_pipeline\u001B[39m\u001B[34m(ds1, ds2, ds3, preds, calib_fit_y)\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     22\u001B[39m     preds_idx = preds[[\u001B[33m\"\u001B[39m\u001B[33mENCODED_MCT\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mTA_YM\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     risks = \u001B[43mrisks\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mENCODED_MCT\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mTA_YM\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mleft\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m     risks[\u001B[33m\"\u001B[39m\u001B[33mp_model\u001B[39m\u001B[33m\"\u001B[39m] = weighted_ensemble(risks)\n\u001B[32m     25\u001B[39m     cal = Calibrator()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/frame.py:10839\u001B[39m, in \u001B[36mDataFrame.merge\u001B[39m\u001B[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[39m\n\u001B[32m  10820\u001B[39m \u001B[38;5;129m@Substitution\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m  10821\u001B[39m \u001B[38;5;129m@Appender\u001B[39m(_merge_doc, indents=\u001B[32m2\u001B[39m)\n\u001B[32m  10822\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmerge\u001B[39m(\n\u001B[32m   (...)\u001B[39m\u001B[32m  10835\u001B[39m     validate: MergeValidate | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m  10836\u001B[39m ) -> DataFrame:\n\u001B[32m  10837\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mreshape\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmerge\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m merge\n\u001B[32m> \u001B[39m\u001B[32m10839\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m  10840\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m  10841\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10843\u001B[39m \u001B[43m        \u001B[49m\u001B[43mon\u001B[49m\u001B[43m=\u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10844\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mleft_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10845\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mright_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10846\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mleft_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10847\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mright_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10848\u001B[39m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10849\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuffixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuffixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10850\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10851\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindicator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindicator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10852\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10853\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/reshape/merge.py:170\u001B[39m, in \u001B[36mmerge\u001B[39m\u001B[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[39m\n\u001B[32m    155\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _cross_merge(\n\u001B[32m    156\u001B[39m         left_df,\n\u001B[32m    157\u001B[39m         right_df,\n\u001B[32m   (...)\u001B[39m\u001B[32m    167\u001B[39m         copy=copy,\n\u001B[32m    168\u001B[39m     )\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m     op = \u001B[43m_MergeOperation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m        \u001B[49m\u001B[43mon\u001B[49m\u001B[43m=\u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mleft_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mright_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mleft_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mright_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuffixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuffixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    181\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindicator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindicator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    183\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    184\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m op.get_result(copy=copy)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/reshape/merge.py:807\u001B[39m, in \u001B[36m_MergeOperation.__init__\u001B[39m\u001B[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001B[39m\n\u001B[32m    803\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_tolerance(\u001B[38;5;28mself\u001B[39m.left_join_keys)\n\u001B[32m    805\u001B[39m \u001B[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001B[39;00m\n\u001B[32m    806\u001B[39m \u001B[38;5;66;03m# to avoid incompatible dtypes\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m807\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_maybe_coerce_merge_keys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    809\u001B[39m \u001B[38;5;66;03m# If argument passed to validate,\u001B[39;00m\n\u001B[32m    810\u001B[39m \u001B[38;5;66;03m# check if columns specified as unique\u001B[39;00m\n\u001B[32m    811\u001B[39m \u001B[38;5;66;03m# are in fact unique.\u001B[39;00m\n\u001B[32m    812\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m validate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/reshape/merge.py:1513\u001B[39m, in \u001B[36m_MergeOperation._maybe_coerce_merge_keys\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1511\u001B[39m \u001B[38;5;66;03m# datetimelikes must match exactly\u001B[39;00m\n\u001B[32m   1512\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m needs_i8_conversion(lk.dtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m needs_i8_conversion(rk.dtype):\n\u001B[32m-> \u001B[39m\u001B[32m1513\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m   1514\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m needs_i8_conversion(lk.dtype) \u001B[38;5;129;01mand\u001B[39;00m needs_i8_conversion(rk.dtype):\n\u001B[32m   1515\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[31mValueError\u001B[39m: You are trying to merge on datetime64[ns] and float64 columns for key 'TA_YM'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "7a94b8b7",
   "metadata": {},
   "source": [
    "### 앙상블 & 확률 보정(Platt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "w = {\"rf\": 0.25, \"gb\": 0.15, \"logit\": 0.25}\n",
    "ws = np.array([w.get(k, 0.0) for k in pred_dict.keys()], dtype=\"float64\")\n",
    "if ws.sum() == 0:\n",
    "    ws = np.ones_like(ws) / len(ws)\n",
    "else:\n",
    "    ws = ws / ws.sum()\n",
    "\n",
    "probs = np.zeros_like(list(pred_dict.values())[0][1])\n",
    "for j, k in enumerate(pred_dict.keys()):\n",
    "    probs += ws[j] * pred_dict[k][1]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pl = LogisticRegression(max_iter=200)\n",
    "pl.fit(probs.reshape(-1, 1), y_test)\n",
    "p_cal = pl.predict_proba(probs.reshape(-1, 1))[:, 1]\n",
    "\n",
    "print(\"Ensemble ROC-AUC (raw):\", roc_auc_score(y_test, probs))\n",
    "print(\"Ensemble ROC-AUC (cal):\", roc_auc_score(y_test, p_cal))\n",
    "print(\"Ensemble PR-AUC (cal):\", average_precision_score(y_test, p_cal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079393f",
   "metadata": {},
   "source": [
    "## preds.csv 생성 (전체 데이터 대상 예측)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_full(pipe_dict, df_full):\n",
    "    Xf = df_full[num_cols + cat_cols].copy()\n",
    "    P = []\n",
    "    keys = list(pipe_dict.keys())\n",
    "    for k in keys:\n",
    "        pipe = pipe_dict[k][0]\n",
    "        p = pipe.predict_proba(Xf)[:, 1] if hasattr(pipe, \"predict_proba\") else pipe.decision_function(Xf)\n",
    "        P.append(p)\n",
    "    P = np.vstack(P).T\n",
    "    m = P.shape[1]\n",
    "    if len(ws) != m:\n",
    "        w2 = np.ones(m) / m\n",
    "    else:\n",
    "        w2 = ws\n",
    "    pe = (P * w2).sum(axis=1)\n",
    "    pc = pl.predict_proba(pe.reshape(-1, 1))[:, 1]\n",
    "    return pc\n",
    "\n",
    "\n",
    "df_full = df[[\"ENCODED_MCT\", \"TA_YM\"]].copy()\n",
    "df_full[\"TA_YM\"] = pd.to_datetime(df_full[\"TA_YM\"], errors=\"coerce\").dt.strftime(\"%Y-%m\")\n",
    "df_full[\"pred_xgb\"] = np.nan\n",
    "df_full[\"pred_lgbm\"] = np.nan\n",
    "df_full[\"pred_rf\"] = predict_full({\"rf\": pred_dict[\"rf\"]}, df)\n",
    "df_full[\"pred_gb\"] = predict_full({\"gb\": pred_dict[\"gb\"]}, df)\n",
    "df_full[\"pred_dl\"] = predict_full({\"logit\": pred_dict[\"logit\"]}, df)\n",
    "\n",
    "preds_path = os.path.join(DATA_DIR, \"preds.csv\")\n",
    "df_full.to_csv(preds_path, index=False, encoding=\"utf-8\")\n",
    "preds_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101c3d7",
   "metadata": {},
   "source": [
    "## 파이프라인 재실행 → 최종 경보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = run_pipeline(ds1, ds2, ds3, preds=read_csv_smart(preds_path))\n",
    "out_path = os.path.join(BASE_DIR, \"risk_output_trained.csv\")\n",
    "out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "out.head(10)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
